{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7705be9-6f8b-49fd-bcca-c07282d0610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\62493\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\62493\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 下载 nltk 需要的资源\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 加载停用词表\n",
    "stop_words = set(stopwords.words('english'))  # 停用词集合\n",
    "\n",
    "def list_files(directory):\n",
    "    \"\"\"获取所有文本文件路径\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"文本预处理：分词、去除标点、小写化、去除数字、停用词过滤、词干提取\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)  # 分词\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub(r'[^\\w\\s]', '', token).lower() # 归一化：去除标点 & 小写化\n",
    "        if token in stop_words: # 跳过停用词\n",
    "            continue\n",
    "        # 词干提取\n",
    "        if token.isalpha():  # 过滤掉纯数字\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            processed_tokens.append(stemmed_token)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d23d6b-4fe4-49ae-8c01-6fe38c80fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "class BSBIIndexer:\n",
    "    def __init__(self, block_size=100000, output_dir=\"bsbi_blocks\"):\n",
    "        self.block_size = block_size\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def index_documents(self, directory):\n",
    "        \"\"\"构建 BSBI 索引\"\"\"\n",
    "        doc_files = list_files(directory)\n",
    "        block_id = 0\n",
    "        term_doc_pairs = []\n",
    "\n",
    "        for doc_id, file_path in enumerate(doc_files):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                tokens = preprocess_text(text)\n",
    "                term_doc_pairs.extend([(token, doc_id) for token in tokens])\n",
    "\n",
    "            if len(term_doc_pairs) >= self.block_size:\n",
    "                self.write_block(term_doc_pairs, block_id)\n",
    "                term_doc_pairs = []\n",
    "                block_id += 1\n",
    "\n",
    "        if term_doc_pairs:\n",
    "            self.write_block(term_doc_pairs, block_id)\n",
    "            block_id += 1\n",
    "\n",
    "        return block_id\n",
    "\n",
    "    def write_block(self, term_doc_pairs, block_id):\n",
    "        \"\"\"对 block 排序并写入磁盘\"\"\"\n",
    "        start_time = time.time();\n",
    "        term_doc_pairs.sort()\n",
    "        block_file = os.path.join(self.output_dir, f\"block_{block_id}.json\")\n",
    "        with open(block_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(term_doc_pairs, f)\n",
    "        end_time = time.time();\n",
    "        print(f\"Block {block_id} written with {len(term_doc_pairs)} pairs. Sorting took {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eea916a-2857-43eb-9403-17422769760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import psutil  # 用于监测内存占用\n",
    "\n",
    "class BSBI_Merger:\n",
    "    def __init__(self, output_dir=\"bsbi_blocks\", final_index=\"final_index.json\"):\n",
    "        self.output_dir = output_dir\n",
    "        self.final_index = final_index\n",
    "\n",
    "    def merge_blocks(self, block_count):\n",
    "        \"\"\"归并多个已排序 block\"\"\"\n",
    "        min_heap = []\n",
    "        file_iters = [open(os.path.join(self.output_dir, f\"block_{i}.json\"), \"r\") for i in range(block_count)]\n",
    "        block_data = [json.load(f) for f in file_iters]\n",
    "\n",
    "        # 记录归并前内存\n",
    "        memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "\n",
    "        # 初始化 heap\n",
    "        for i, block in enumerate(block_data):\n",
    "            if block:\n",
    "                heapq.heappush(min_heap, (block[0][0], block[0][1], i, 0))  # (term, doc_id, block_index, position)\n",
    "\n",
    "        merged_index = {}\n",
    "\n",
    "        while min_heap:\n",
    "            term, doc_id, block_idx, pos = heapq.heappop(min_heap)\n",
    "\n",
    "            if term not in merged_index:\n",
    "                merged_index[term] = set()\n",
    "            merged_index[term].add(doc_id)\n",
    "\n",
    "            # 继续从相应的 block 取数据\n",
    "            if pos + 1 < len(block_data[block_idx]):\n",
    "                next_term, next_doc_id = block_data[block_idx][pos + 1]\n",
    "                heapq.heappush(min_heap, (next_term, next_doc_id, block_idx, pos + 1))\n",
    "\n",
    "        # 记录归并后内存\n",
    "        memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "        \n",
    "        # 关闭文件\n",
    "        for f in file_iters:\n",
    "            f.close()\n",
    "\n",
    "        # 写入最终索引\n",
    "        with open(self.final_index, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({term: sorted(list(doc_ids)) for term, doc_ids in merged_index.items()}, f)\n",
    "\n",
    "\n",
    "        print(f\"Pre-merge memory: {memory_before:.2f} MB | Post-merge memory: {memory_after:.2f} MB | Increase: {memory_after - memory_before:.2f} MB\\n\")\n",
    "        print(f\"Final index written to {self.final_index}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d3d8c7-e7c3-4fc3-8c78-39a98ea0b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 written with 101237 pairs. Sorting took 0.18 seconds.\n",
      "Block 1 written with 100201 pairs. Sorting took 0.20 seconds.\n",
      "Block 2 written with 100026 pairs. Sorting took 0.30 seconds.\n",
      "Block 3 written with 100445 pairs. Sorting took 0.18 seconds.\n",
      "Block 4 written with 100048 pairs. Sorting took 0.18 seconds.\n",
      "Block 5 written with 100093 pairs. Sorting took 0.20 seconds.\n",
      "Block 6 written with 101953 pairs. Sorting took 0.20 seconds.\n",
      "Block 7 written with 100076 pairs. Sorting took 0.28 seconds.\n",
      "Block 8 written with 100046 pairs. Sorting took 0.31 seconds.\n",
      "Block 9 written with 101223 pairs. Sorting took 0.33 seconds.\n",
      "Block 10 written with 100052 pairs. Sorting took 0.22 seconds.\n",
      "Block 11 written with 102402 pairs. Sorting took 0.21 seconds.\n",
      "Block 12 written with 100134 pairs. Sorting took 0.28 seconds.\n",
      "Block 13 written with 100414 pairs. Sorting took 0.21 seconds.\n",
      "Block 14 written with 100046 pairs. Sorting took 0.20 seconds.\n",
      "Block 15 written with 100091 pairs. Sorting took 0.27 seconds.\n",
      "Block 16 written with 100036 pairs. Sorting took 0.23 seconds.\n",
      "Block 17 written with 2425 pairs. Sorting took 0.01 seconds.\n",
      "\n",
      "Indexing completed in 46.42 seconds.\n",
      "\n",
      "Pre-merge memory: 463.64 MB | Post-merge memory: 522.39 MB | Increase: 58.75 MB\n",
      "\n",
      "Final index written to final_index.json\n",
      "\n",
      "Merge completed in 2.60 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"dataset/HillaryEmails\"  # 你的数据集目录\n",
    "    block_size = 100000  # 约为1.2MB,每个 block 处理的最大 term-document 对数\n",
    "\n",
    "    # 1. 运行 BSBI 索引构建\n",
    "    indexer = BSBIIndexer(block_size=block_size)\n",
    "    start_time = time.time()\n",
    "    block_count = indexer.index_documents(input_directory)\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nIndexing completed in {end_time - start_time:.2f} seconds.\\n\")\n",
    "\n",
    "    # 2. 归并所有 block\n",
    "    merger = BSBI_Merger()\n",
    "    start_time = time.time()\n",
    "    merger.merge_blocks(block_count)\n",
    "    end_time = time.time()\n",
    "    print(f\"Merge completed in {end_time - start_time:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
