{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipUt7xgua7vY",
        "outputId": "878865be-41a8-4549-959e-0fff01d95a88"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import struct\n",
        "import math\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "sY9HQcOTeriF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# 下载 nltk 需要的资源\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 加载停用词表\n",
        "stop_words = set(stopwords.words('english'))  # 停用词集合\n",
        "\n",
        "def list_files(directory):\n",
        "    \"\"\"获取所有文本文件路径\"\"\"\n",
        "    return [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"文本预处理：分词、去除标点、小写化、去除数字、停用词过滤、词干提取\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = nltk.word_tokenize(text)  # 分词\n",
        "    processed_tokens = []\n",
        "    for token in tokens:\n",
        "        token = re.sub(r'[^\\w\\s]', '', token).lower() # 归一化：去除标点 & 小写化\n",
        "        if token in stop_words: # 跳过停用词\n",
        "            continue\n",
        "        # 词干提取\n",
        "        if token.isalpha():  # 过滤掉纯数字\n",
        "            stemmed_token = stemmer.stem(token)\n",
        "            processed_tokens.append(stemmed_token)\n",
        "    return processed_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYknR_48f0oy",
        "outputId": "2e174fdf-4c36-4358-d756-de4364b71056"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "class BSBIIndexer:\n",
        "    def __init__(self, block_size=100000, output_dir=\"bsbi_blocks\"):\n",
        "        self.block_size = block_size\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def index_documents(self, directory):\n",
        "        \"\"\"构建 BSBI 索引\"\"\"\n",
        "        doc_files = list_files(directory)\n",
        "        block_id = 0\n",
        "        term_doc_pairs = []\n",
        "\n",
        "        for doc_id, file_path in enumerate(doc_files):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "                tokens = preprocess_text(text)\n",
        "                term_doc_pairs.extend([(token, doc_id) for token in tokens])\n",
        "\n",
        "            if len(term_doc_pairs) >= self.block_size:\n",
        "                self.write_block(term_doc_pairs, block_id)\n",
        "                term_doc_pairs = []\n",
        "                block_id += 1\n",
        "\n",
        "        if term_doc_pairs:\n",
        "            self.write_block(term_doc_pairs, block_id)\n",
        "            block_id += 1\n",
        "\n",
        "        return block_id\n",
        "\n",
        "    def write_block(self, term_doc_pairs, block_id):\n",
        "        \"\"\"对 block 排序并写入磁盘\"\"\"\n",
        "        start_time = time.time();\n",
        "        term_doc_pairs.sort()\n",
        "        block_file = os.path.join(self.output_dir, f\"block_{block_id}.json\")\n",
        "        with open(block_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(term_doc_pairs, f)\n",
        "        end_time = time.time();\n",
        "        print(f\"Block {block_id} written with {len(term_doc_pairs)} pairs. Sorting took {end_time - start_time:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "GMOmxeYngPgi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "import psutil  # 用于监测内存占用\n",
        "\n",
        "class BSBI_Merger:\n",
        "    def __init__(self, output_dir=\"bsbi_blocks\", final_index=\"final_index.json\"):\n",
        "        self.output_dir = output_dir\n",
        "        self.final_index = final_index\n",
        "\n",
        "    def merge_blocks(self, block_count):\n",
        "        \"\"\"归并多个已排序 block\"\"\"\n",
        "        min_heap = []\n",
        "        file_iters = [open(os.path.join(self.output_dir, f\"block_{i}.json\"), \"r\") for i in range(block_count)]\n",
        "        block_data = [json.load(f) for f in file_iters]\n",
        "\n",
        "        # 记录归并前内存\n",
        "        memory_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "\n",
        "        # 初始化 heap\n",
        "        for i, block in enumerate(block_data):\n",
        "            if block:\n",
        "                heapq.heappush(min_heap, (block[0][0], block[0][1], i, 0))  # (term, doc_id, block_index, position)\n",
        "\n",
        "        merged_index = {}\n",
        "\n",
        "        while min_heap:\n",
        "            term, doc_id, block_idx, pos = heapq.heappop(min_heap)\n",
        "\n",
        "            if term not in merged_index:\n",
        "                merged_index[term] = set()\n",
        "            merged_index[term].add(doc_id)\n",
        "\n",
        "            # 继续从相应的 block 取数据\n",
        "            if pos + 1 < len(block_data[block_idx]):\n",
        "                next_term, next_doc_id = block_data[block_idx][pos + 1]\n",
        "                heapq.heappush(min_heap, (next_term, next_doc_id, block_idx, pos + 1))\n",
        "\n",
        "        # 记录归并后内存\n",
        "        memory_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "\n",
        "        # 关闭文件\n",
        "        for f in file_iters:\n",
        "            f.close()\n",
        "\n",
        "        # 写入最终索引\n",
        "        with open(self.final_index, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({term: sorted(list(doc_ids)) for term, doc_ids in merged_index.items()}, f)\n",
        "\n",
        "\n",
        "        print(f\"Pre-merge memory: {memory_before:.2f} MB | Post-merge memory: {memory_after:.2f} MB | Increase: {memory_after - memory_before:.2f} MB\\n\")\n",
        "        print(f\"Final index written to {self.final_index}\\n\")"
      ],
      "metadata": {
        "id": "SgdGuMuLgSVT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Compression Techniques\n",
        "1. **Dictionary Compression**: Reduce storage overhead of string terms by mapping them to numeric IDs.\n",
        "2. **Delta Encoding**: Compress sorted document ID lists by storing differences between consecutive IDs.\n",
        "3. **Variable Byte (VByte) Encoding**: Compact representation of integers using variable-length bytes."
      ],
      "metadata": {
        "id": "ObTIiqCfnKEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CompressedIndex:\n",
        "    def __init__(self, original_index_file):\n",
        "        \"\"\"初始化压缩索引\"\"\"\n",
        "        self.original_index_file = original_index_file\n",
        "        self.compressed_index_file = original_index_file.replace('.json', '.compressed')\n",
        "        self.term_dict_file = original_index_file.replace('.json', '.term_dict')\n",
        "\n",
        "        # 加载原始索引\n",
        "        with open(original_index_file, 'r', encoding='utf-8') as f:\n",
        "            self.original_index = json.load(f)\n",
        "\n",
        "        # 压缩后的数据结构\n",
        "        self.term_to_id = {}  # 术语到ID的映射\n",
        "        self.id_to_term = {}   # ID到术语的映射\n",
        "        self.compressed_index = {}  # 压缩后的索引 {term_id: compressed_doc_ids}\n",
        "\n",
        "        # 统计信息\n",
        "        self.original_size = 0\n",
        "        self.compressed_size = 0\n",
        "\n",
        "    def build_term_dictionary(self):\n",
        "        \"\"\"构建术语字典，为每个术语分配一个唯一ID\"\"\"\n",
        "        sorted_terms = sorted(self.original_index.keys())\n",
        "        for term_id, term in enumerate(sorted_terms):\n",
        "            self.term_to_id[term] = term_id\n",
        "            self.id_to_term[term_id] = term\n",
        "\n",
        "    def variable_byte_encode(self, numbers):\n",
        "        \"\"\"变长字节编码压缩数字列表\"\"\"\n",
        "        encoded_bytes = bytearray()\n",
        "        for num in numbers:\n",
        "            # 处理0的特殊情况\n",
        "            if num == 0:\n",
        "                encoded_bytes.append(0x80)\n",
        "                continue\n",
        "\n",
        "            bytes_to_write = []\n",
        "            while num > 0:\n",
        "                bytes_to_write.append(num & 0x7F)\n",
        "                num >>= 7\n",
        "\n",
        "            # 设置最高位标记字节结束\n",
        "            bytes_to_write[0] |= 0x80\n",
        "            # 反转字节顺序\n",
        "            encoded_bytes.extend(reversed(bytes_to_write))\n",
        "        return bytes(encoded_bytes)\n",
        "\n",
        "    def variable_byte_decode(self, encoded_bytes):\n",
        "        \"\"\"变长字节解码\"\"\"\n",
        "        numbers = []\n",
        "        current_num = 0\n",
        "        for byte in encoded_bytes:\n",
        "            # 检查最高位\n",
        "            if byte & 0x80:\n",
        "                current_num = (current_num << 7) | (byte & 0x7F)\n",
        "                numbers.append(current_num)\n",
        "                current_num = 0\n",
        "            else:\n",
        "                current_num = (current_num << 7) | byte\n",
        "        return numbers\n",
        "\n",
        "    def delta_encode(self, numbers):\n",
        "        \"\"\"Delta编码\"\"\"\n",
        "        if not numbers:\n",
        "            return []\n",
        "\n",
        "        # 先排序\n",
        "        sorted_numbers = sorted(numbers)\n",
        "        delta_encoded = [sorted_numbers[0]]\n",
        "        for i in range(1, len(sorted_numbers)):\n",
        "            delta_encoded.append(sorted_numbers[i] - sorted_numbers[i-1])\n",
        "        return delta_encoded\n",
        "\n",
        "    def delta_decode(self, delta_encoded):\n",
        "        \"\"\"Delta解码\"\"\"\n",
        "        if not delta_encoded:\n",
        "            return []\n",
        "\n",
        "        numbers = [delta_encoded[0]]\n",
        "        for i in range(1, len(delta_encoded)):\n",
        "            numbers.append(numbers[-1] + delta_encoded[i])\n",
        "        return numbers\n",
        "\n",
        "    def compress_index(self):\n",
        "        \"\"\"压缩整个索引\"\"\"\n",
        "        self.build_term_dictionary()\n",
        "\n",
        "        for term, doc_ids in self.original_index.items():\n",
        "            term_id = self.term_to_id[term]\n",
        "\n",
        "            # 先进行delta编码\n",
        "            delta_encoded = self.delta_encode(doc_ids)\n",
        "\n",
        "            # 再进行变长字节编码\n",
        "            compressed_doc_ids = self.variable_byte_encode(delta_encoded)\n",
        "\n",
        "            # 存储压缩后的数据\n",
        "            self.compressed_index[term_id] = compressed_doc_ids\n",
        "\n",
        "    def decompress_term(self, term_id):\n",
        "        \"\"\"解压缩单个术语的文档列表\"\"\"\n",
        "        compressed_data = self.compressed_index.get(term_id, b'')\n",
        "        if not compressed_data:\n",
        "            return []\n",
        "\n",
        "        # 先变长字节解码\n",
        "        delta_encoded = self.variable_byte_decode(compressed_data)\n",
        "\n",
        "        # 再delta解码\n",
        "        doc_ids = self.delta_decode(delta_encoded)\n",
        "\n",
        "        return doc_ids\n",
        "\n",
        "    def save_compressed_index(self):\n",
        "        \"\"\"保存压缩后的索引到文件\"\"\"\n",
        "        # 保存术语字典\n",
        "        with open(self.term_dict_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'term_to_id': self.term_to_id,\n",
        "                'id_to_term': self.id_to_term\n",
        "            }, f)\n",
        "\n",
        "        # 保存压缩后的索引\n",
        "        with open(self.compressed_index_file, 'wb') as f:\n",
        "            # 先写入术语数量\n",
        "            f.write(struct.pack('I', len(self.compressed_index)))\n",
        "\n",
        "            for term_id, compressed_data in self.compressed_index.items():\n",
        "                # 写入term_id (4字节)\n",
        "                f.write(struct.pack('I', term_id))\n",
        "                # 写入压缩数据长度 (4字节)\n",
        "                f.write(struct.pack('I', len(compressed_data)))\n",
        "                # 写入压缩数据\n",
        "                f.write(compressed_data)\n",
        "\n",
        "    def load_compressed_index(self):\n",
        "        \"\"\"从文件加载压缩索引\"\"\"\n",
        "        # 加载术语字典\n",
        "        with open(self.term_dict_file, 'r', encoding='utf-8') as f:\n",
        "            dict_data = json.load(f)\n",
        "            self.term_to_id = dict_data['term_to_id']\n",
        "            self.id_to_term = dict_data['id_to_term']\n",
        "\n",
        "        # 加载压缩索引\n",
        "        self.compressed_index = {}\n",
        "        with open(self.compressed_index_file, 'rb') as f:\n",
        "            # 读取术语数量\n",
        "            term_count = struct.unpack('I', f.read(4))[0]\n",
        "\n",
        "            for _ in range(term_count):\n",
        "                # 读取term_id\n",
        "                term_id = struct.unpack('I', f.read(4))[0]\n",
        "                # 读取数据长度\n",
        "                data_len = struct.unpack('I', f.read(4))[0]\n",
        "                # 读取压缩数据\n",
        "                compressed_data = f.read(data_len)\n",
        "\n",
        "                self.compressed_index[term_id] = compressed_data\n",
        "\n",
        "    def calculate_sizes(self):\n",
        "        \"\"\"计算压缩前后的尺寸\"\"\"\n",
        "        # 原始索引大小\n",
        "        self.original_size = os.path.getsize(self.original_index_file)\n",
        "\n",
        "        # 压缩后大小\n",
        "        self.compress_index()\n",
        "        self.save_compressed_index()\n",
        "        self.compressed_size = os.path.getsize(self.compressed_index_file) + \\\n",
        "                             os.path.getsize(self.term_dict_file)\n",
        "\n",
        "        return {\n",
        "            'original_size': self.original_size,\n",
        "            'compressed_size': self.compressed_size,\n",
        "            'compression_ratio': self.compressed_size / self.original_size\n",
        "        }\n",
        "\n",
        "    def print_compression_stats(self):\n",
        "        \"\"\"打印压缩统计信息\"\"\"\n",
        "        stats = self.calculate_sizes()\n",
        "        print(\"\\nCompression Statistics:\")\n",
        "        print(f\"Original index size: {stats['original_size'] / 1024:.2f} KB\")\n",
        "        print(f\"Compressed index size: {stats['compressed_size'] / 1024:.2f} KB\")\n",
        "        print(f\"Compression ratio: {stats['compression_ratio']:.2%}\")\n",
        "        print(f\"Space savings: {(1 - stats['compression_ratio']) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "qs-o0MtFfY3v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CompressedBooleanQueryProcessor:\n",
        "    def __init__(self, compressed_index):\n",
        "        self.compressed_index = compressed_index\n",
        "\n",
        "    def process_query(self, query):\n",
        "      \"\"\" 处理布尔查询（完整实现）\n",
        "\n",
        "      支持 AND, OR, NOT 运算符，运算符优先级：NOT > AND > OR\n",
        "      示例:\n",
        "          \"word1 AND word2 OR word3\" -> (word1 AND word2) OR word3\n",
        "          \"word1 OR word2 AND word3\" -> word1 OR (word2 AND word3)\n",
        "          \"word1 AND NOT word2\" -> word1 AND (NOT word2)\n",
        "      \"\"\"\n",
        "      terms = query.split()\n",
        "      if not terms:\n",
        "          return []\n",
        "\n",
        "      # 初始化栈用于处理运算符优先级\n",
        "      operand_stack = []\n",
        "      operator_stack = []\n",
        "\n",
        "      i = 0\n",
        "      while i < len(terms):\n",
        "          term = terms[i]\n",
        "\n",
        "          if term.upper() in {'AND', 'OR', 'NOT'}:\n",
        "              # 处理运算符\n",
        "              current_op = term.upper()\n",
        "\n",
        "              # NOT 运算符是单目运算符，直接处理\n",
        "              if current_op == 'NOT':\n",
        "                  # 获取下一个词作为操作数\n",
        "                  if i + 1 >= len(terms):\n",
        "                      raise ValueError(\"NOT operator must be followed by a term\")\n",
        "\n",
        "                  next_term = terms[i+1].lower()\n",
        "                  if next_term in {'AND', 'OR', 'NOT'}:\n",
        "                      raise ValueError(f\"Invalid term after NOT: {next_term}\")\n",
        "\n",
        "                  # 处理NOT操作\n",
        "                  term_id = self.compressed_index.term_to_id.get(next_term, -1)\n",
        "                  if term_id == -1:\n",
        "                      doc_set = set()  # 词不存在，NOT后为空集\n",
        "                  else:\n",
        "                      doc_set = set(self.compressed_index.decompress_term(term_id))\n",
        "\n",
        "                  # NOT操作就是取补集（这里需要知道全集，简化处理为当前结果集的补集）\n",
        "                  # 实际应用中应该传入全集文档数\n",
        "                  operand_stack.append(doc_set)\n",
        "                  i += 2  # 跳过已处理的NOT和下一个词\n",
        "              else:\n",
        "                  # AND 或 OR 运算符\n",
        "                  operator_stack.append(current_op)\n",
        "                  i += 1\n",
        "          else:\n",
        "              # 处理普通词项\n",
        "              term = term.lower()\n",
        "              term_id = self.compressed_index.term_to_id.get(term, -1)\n",
        "              if term_id == -1:\n",
        "                  doc_set = set()  # 词不存在\n",
        "              else:\n",
        "                  doc_set = set(self.compressed_index.decompress_term(term_id))\n",
        "              operand_stack.append(doc_set)\n",
        "              i += 1\n",
        "\n",
        "      # 如果没有运算符，直接返回第一个操作数\n",
        "      if not operator_stack:\n",
        "          return sorted(operand_stack[0]) if operand_stack else []\n",
        "\n",
        "      # 按照运算符优先级处理栈中的内容 (NOT已经处理，剩下AND优先级高于OR)\n",
        "      result = operand_stack[0]\n",
        "      for i in range(len(operator_stack)):\n",
        "          op = operator_stack[i]\n",
        "          next_operand = operand_stack[i+1]\n",
        "\n",
        "          if op == 'AND':\n",
        "              result &= next_operand\n",
        "          elif op == 'OR':\n",
        "              result |= next_operand\n",
        "\n",
        "      return sorted(result)"
      ],
      "metadata": {
        "id": "7f_hjDBYfd9z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class BooleanQueryProcessor:\n",
        "    def __init__(self, index_file):\n",
        "        with open(index_file, 'r', encoding='utf-8') as f:\n",
        "            self.inverted_index = json.load(f)\n",
        "\n",
        "    def process_query(self, query):\n",
        "        # 将查询字符串拆分为操作符和操作数\n",
        "        terms = query.split()\n",
        "        if not terms:\n",
        "            return []\n",
        "\n",
        "        result_set = None\n",
        "        current_op = None\n",
        "\n",
        "        for term in terms:\n",
        "            if term.upper() in {'AND', 'OR', 'NOT'}:\n",
        "                current_op = term.upper()\n",
        "            else:\n",
        "                term = term.lower()\n",
        "                if term in self.inverted_index:\n",
        "                    doc_set = set(self.inverted_index[term])\n",
        "                else:\n",
        "                    doc_set = set()\n",
        "\n",
        "                if result_set is None:\n",
        "                    result_set = doc_set\n",
        "                else:\n",
        "                    if current_op == 'AND':\n",
        "                        result_set &= doc_set\n",
        "                    elif current_op == 'OR':\n",
        "                        result_set |= doc_set\n",
        "                    elif current_op == 'NOT':\n",
        "                        result_set -= doc_set\n",
        "\n",
        "        return sorted(result_set) if result_set is not None else []"
      ],
      "metadata": {
        "id": "TTi9WJaNlWKk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # base_dir = os.path.expanduser(\"~/Documents/HillaryEmails\")\n",
        "    base_dir = \"/content/drive/MyDrive/HillaryEmails/HillaryEmails\"\n",
        "    input_dir = base_dir  # 你的数据集目录\n",
        "    block_size = 100000  # 约为1.2MB,每个 block 处理的最大 term-document 对数\n",
        "\n",
        "    # 原有索引构建（Task 1-2）\n",
        "    indexer = BSBIIndexer()\n",
        "    block_count = indexer.index_documents(input_dir)\n",
        "    merger = BSBI_Merger()\n",
        "    merger.merge_blocks(block_count)\n",
        "\n",
        "    # 新增压缩步骤（Task 3）\n",
        "    compressed_index = CompressedIndex(\"/content/final_index.json\")\n",
        "    compressed_index.compress_index()\n",
        "    compressed_index.save_compressed_index()\n",
        "    compressed_index.print_compression_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp6fBdrUgZtM",
        "outputId": "ce7660ca-b118-42e1-87b2-90c3a970fadf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block 0 written with 100270 pairs. Sorting took 0.51 seconds.\n",
            "Block 1 written with 100934 pairs. Sorting took 0.31 seconds.\n",
            "Block 2 written with 100068 pairs. Sorting took 0.50 seconds.\n",
            "Block 3 written with 100943 pairs. Sorting took 0.31 seconds.\n",
            "Block 4 written with 100048 pairs. Sorting took 0.29 seconds.\n",
            "Block 5 written with 100076 pairs. Sorting took 0.30 seconds.\n",
            "Block 6 written with 100108 pairs. Sorting took 0.30 seconds.\n",
            "Block 7 written with 100084 pairs. Sorting took 0.54 seconds.\n",
            "Block 8 written with 100450 pairs. Sorting took 0.30 seconds.\n",
            "Block 9 written with 2503 pairs. Sorting took 0.01 seconds.\n",
            "Pre-merge memory: 518.53 MB | Post-merge memory: 526.78 MB | Increase: 8.25 MB\n",
            "\n",
            "Final index written to final_index.json\n",
            "\n",
            "\n",
            "Compression Statistics:\n",
            "Original index size: 3072.05 KB\n",
            "Compressed index size: 1772.34 KB\n",
            "Compression ratio: 57.69%\n",
            "Space savings: 42.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "def run_query_examples(use_compressed=True):\n",
        "    \"\"\"执行查询示例并比较结果\"\"\"\n",
        "    # 加载索引（根据选择加载压缩版或原始版）\n",
        "    if use_compressed:\n",
        "        print(\"=== 使用压缩版索引查询 ===\")\n",
        "        compressed_index = CompressedIndex(\"final_index.json\")\n",
        "        compressed_index.load_compressed_index()  # 加载已压缩的索引\n",
        "        processor = CompressedBooleanQueryProcessor(compressed_index)\n",
        "    else:\n",
        "        print(\"=== 使用原始版索引查询 ===\")\n",
        "        processor = BooleanQueryProcessor(\"final_index.json\")  # 直接加载原始JSON索引\n",
        "\n",
        "    # 测试查询列表\n",
        "    test_queries = [\n",
        "        \"clinton AND email\",          # 简单AND查询\n",
        "        \"obama OR president\",         # 简单OR查询\n",
        "        \"hillary NOT bill\",           # NOT查询\n",
        "        \"security AND (white OR house)\",  # 组合查询\n",
        "        \"nonexistent_term\"            # 不存在的词项\n",
        "    ]\n",
        "\n",
        "    # 执行每个查询\n",
        "    for query in test_queries:\n",
        "        print(f\"\\n查询: '{query}'\")\n",
        "\n",
        "        # 计时\n",
        "        start_time = time()\n",
        "        result = processor.process_query(query)\n",
        "        elapsed_ms = (time() - start_time) * 1000  # 毫秒\n",
        "\n",
        "        # 显示结果\n",
        "        print(f\"返回文档数: {len(result)}\")\n",
        "        print(f\"耗时: {elapsed_ms:.2f} ms\")\n",
        "        if result:\n",
        "            print(f\"前5个文档ID示例: {result[:5]}\")\n",
        "\n",
        "# 对比测试\n",
        "print(\"【性能对比测试】\")\n",
        "run_query_examples(use_compressed=True)    # 压缩版查询\n",
        "run_query_examples(use_compressed=False)   # 原始版查询"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0f3DWUbhkWo",
        "outputId": "bef4db96-2c92-4722-c108-3e15a51e86fe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "【性能对比测试】\n",
            "=== 使用压缩版索引查询 ===\n",
            "\n",
            "查询: 'clinton AND email'\n",
            "返回文档数: 119\n",
            "耗时: 0.32 ms\n",
            "前5个文档ID示例: [57, 73, 241, 266, 267]\n",
            "\n",
            "查询: 'obama OR president'\n",
            "返回文档数: 377\n",
            "耗时: 0.13 ms\n",
            "前5个文档ID示例: [13, 16, 57, 59, 101]\n",
            "\n",
            "查询: 'hillary NOT bill'\n",
            "返回文档数: 0\n",
            "耗时: 0.08 ms\n",
            "\n",
            "查询: 'security AND (white OR house)'\n",
            "返回文档数: 0\n",
            "耗时: 0.01 ms\n",
            "\n",
            "查询: 'nonexistent_term'\n",
            "返回文档数: 0\n",
            "耗时: 0.00 ms\n",
            "=== 使用原始版索引查询 ===\n",
            "\n",
            "查询: 'clinton AND email'\n",
            "返回文档数: 119\n",
            "耗时: 0.13 ms\n",
            "前5个文档ID示例: [57, 73, 241, 266, 267]\n",
            "\n",
            "查询: 'obama OR president'\n",
            "返回文档数: 377\n",
            "耗时: 0.06 ms\n",
            "前5个文档ID示例: [13, 16, 57, 59, 101]\n",
            "\n",
            "查询: 'hillary NOT bill'\n",
            "返回文档数: 0\n",
            "耗时: 0.03 ms\n",
            "\n",
            "查询: 'security AND (white OR house)'\n",
            "返回文档数: 0\n",
            "耗时: 0.01 ms\n",
            "\n",
            "查询: 'nonexistent_term'\n",
            "返回文档数: 0\n",
            "耗时: 0.00 ms\n"
          ]
        }
      ]
    }
  ]
}